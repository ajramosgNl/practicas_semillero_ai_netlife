{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Practica # 1:**\n",
    "## **Ense√±ando a la IA a contar chistes**\n",
    "\n",
    "Los modelos de lenguaje entienden texto, pero‚Ä¶ ¬øentienden el humor? üòÖ\n",
    "\n",
    "En esta pr√°ctica vas a experimentar con modelos de Hugging Face para intentar que un Transformer genere chistes divertidos y coherentes.\n",
    "Tu misi√≥n es usar diferentes modelos y t√©cnicas de prompting para obtener el mejor chiste posible.\n",
    "\n",
    "\n",
    "### **üìã Requisitos**\n",
    "* Instalar las siguiente librerias: `pip install \"torch>=2.3.0\" \"transformers==4.44.2\" \"huggingface_hub==0.25.2\" \"accelerate==0.33.0\" \"safetensors==0.4.4\"`\n",
    "\n",
    "* Cuenta en hugginface\n",
    "\n",
    "* Crear un token de hugginface\n",
    "\n",
    "\n",
    "### **üéØ Objetivos**\n",
    "\n",
    "* Probar modelos de generaci√≥n de texto `(mrm8488/spanish-gpt2, datificate/gpt2-small-spanish, etc.)`.\n",
    "\n",
    "* Aplicar las t√©cnicas de prompting para mejorar la calidad y coherencia del chiste.\n",
    "\n",
    "* Analizar c√≥mo afectan los par√°metros temperature, top_p y max_new_tokens a la creatividad del modelo.\n",
    "\n",
    "* Documentar cu√°l fue tu mejor prompt y por qu√© funcion√≥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajramosg/Desarrollo/Demos/PRACTICAS SEMILLERO/PRACTICA 1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/ajramosg/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.68s/it]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "token = \"token_id\"\n",
    "login(token)\n",
    "\n",
    "# modelo = \"datificate/gpt2-small-spanish\"\n",
    "# modelo = \"DeepESP/gpt2-spanish\"\n",
    "# modelo = \"flax-community/gpt-2-spanish\"\n",
    "modelo = \"google/gemma-2b-it\"\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "modelo_chistes = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=modelo, \n",
    "    device_map=None,     # evita offload\n",
    "    device=\"cpu\",        # fuerza CPU\n",
    "    torch_dtype=\"float16\"  # reduce uso de memoria\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajramosg/Desarrollo/Demos/PRACTICAS SEMILLERO/PRACTICA 1/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ajramosg/Desarrollo/Demos/PRACTICAS SEMILLERO/PRACTICA 1/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≠ Prompt:\n",
      "Cu√©ntame un chiste corto sobre inteligencia artificial.\n",
      "\n",
      "ü§£ Chiste generado:\n",
      "Cu√©ntame un chiste corto sobre inteligencia artificial.\n",
      "\n",
      "¬øQu√© es la inteligencia artificial que no puede ser programada?\n",
      "\n",
      "Una respuesta artificial.\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Definir un prompt\n",
    "prompt = \"\"\"Cu√©ntame un chiste corto sobre inteligencia artificial.\"\"\"\n",
    "\n",
    "# Paso 2: Generar el chiste con el modelo\n",
    "resultado = modelo_chistes(\n",
    "    prompt,\n",
    "    max_new_tokens=80,   # longitud m√°xima del texto generado\n",
    "    temperature=0.9,     # controla la creatividad\n",
    "    top_p=0.95           # controla la diversidad\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "# Paso 3: Mostrar el resultado\n",
    "print(\"üé≠ Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\nü§£ Chiste generado:\")\n",
    "print(resultado)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
